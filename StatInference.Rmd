---
title: "STATISTICAL INFERENCE"
subtitle: "and confidence intervals"
author: "Austin Hart"
institute: "American University"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: [default, rladies, rladies-fonts]
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
---


```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  message=FALSE, warning=FALSE, eval = TRUE, echo = FALSE, 
  fig.align = 'center', dev='svglite'
)
```

```{r results='hide'}

library(tidyverse)

load('DCPS testing.RData')

```

<style type="text/css">
.remark-slide-content {
    font-size: 25px;
    padding: 1em 4em 1em 4em;
}
</style>


# Statistical inference

> Process of using sample stats to learn about properties of a population.
  
- Used for 
  - describing an unobserved/unobservable population  
  - differentiating systematic vs random patterns
  
- Two approaches to statistical inference:  
  - Confidence intervals: construct a plausible range around your estimate.   
  - Hypothesis testing: calculate probability of observing your sample stat given some assumption about the population.  



---
# Sampling variability and the need for inference

- **Population**: entire 'world' of units the study aims to learn about
- *Parameter*: characteristics of the population, e.g., $\mu_y$  

- **Sample**: 'slice' of data from the population
- *Sample statistic*: characteristic of sampled units, e.g., $\bar{y}$ or $Avg[y_i]$  

- **Sampling Variability**: Sample stats are variable.
  - Different across samples: $\bar{y}_1 \neq \bar{y}_2$   
  - Different from target parameter: $\bar{y}_1 \neq \mu_y$


---
# Sampling variability in action

### DCPS PARCC: math proficiency

.pull-left[
Population parameters
- Pop size (N) = 108  
- $\mu_y$ = `r round(mean(dcps$ProfMath),1)`  

]

.pull-right[
```{r pop, fig.width=4, fig.height=3, dpi=200}
  hist(dcps$ProfMath,
       main = "Math Proficiency, DCPS 2017",
       xlab = "% testing at grade level", 
       xlim = c(0,100), col="#69b3a2")
  abline(v = mean(dcps$ProfMath), col = 'red')
```

]

---
# Take a sample...

```{r samples}
s1 = pull(dcps, ProfMath) %>% sample(size = 25)
s2 = pull(dcps, ProfMath) %>% sample(size = 25)
```

.pull-left[
### Random Sample 1 (n = 25)
- $\bar{y}_1$ = `r round(mean(s1),1)`

```{r s1, fig.width=4, fig.height=3, dpi=200}
  hist(s1,
       main = "Sample 1",
       xlab = "% testing at grade level", 
       xlim = c(0,100), col="#69b3a2",
       ylim = c(0,10))
  abline(v = mean(s1), col = 'red')
```

]

.pull-right[
### Random Sample 2 (n = 25)
- $\bar{y}_2$ = `r round(mean(s2),1)`

```{r s2, fig.width=4, fig.height=3, dpi=200}
  hist(s2,
       main = "Sample 2",
       xlab = "% testing at grade level", 
       xlim = c(0,100), col="#69b3a2",
       ylim = c(0,10))
  abline(v = mean(s2), col = 'red')
```

]


---
class: inverse, middle

# INTERLUDE: NORMAL DISTRIBUTIONS


---
# Step 1: standard scores

- $Z$ score: distance from the mean expressed as number of standard deviations

$$
Z_i = \frac{y_i - \bar{y}}{s_y}
$$
- Eileen scored a 27 on the ACT. Nina scored 1300 on the SAT. Who did better?
  - Mean ACT score = 21, sd = 5
  - Mean SAT score = 1050, sd = 300



---
# Step 2: normal (Z) distribution

- A continuous probability distribution for a random variable.  
- 68-95-99 rule
- How does curve change with different sd?

```{r normaldist, fig.width=4, fig.height=3, dpi=350}
  ggplot(NULL, aes(c(-3,3))) +
    geom_area(stat = "function", fun = dnorm, fill = "#00998a", xlim = c(-4, 4)) +
    labs(x = "Distance from mean (Z)", y = "Probability") +
    scale_y_continuous(breaks = NULL,
                       expand = expansion(mult=c(0,0.05))) +
    scale_x_continuous(breaks = c(-3:3)) +
    coord_cartesian(xlim = c(-3.1,3.1)) +
    theme_minimal() +
    theme(panel.grid.minor = element_blank())
```
  
  

---
class: inverse, middle

# SAMPLING DISTRIBUTIONS

### And the Central Limit Theorem


---
# ALL the sample statistics

> What if we had access to thousands of samples?


- Select a random sample of 25 schools.  

- Calculate the mean math proficiency.  

- Repeat 10,000 times.  

- Plot the 10,001 sample means.


---
# Meet the sampling distribution

> Sampling distribution of sample means: probability distribution of possible sample means from a population.


```{r draws}
l = NULL

for (i in 1:10000) {
  l[i] = pull(dcps, ProfMath) %>% sample(size = 25) %>% mean()
}
  
```

.pull-left[

```{r sdist, fig.width=3.5, fig.height=3.5, dpi=200}

  hist(l,
       main = "10,000 sample means",
       xlab = "Sample mean", 
       col="#69b3a2")
  abline(v = mean(l), col = 'red')
  
```
]

.pull-right[  
Our sampling distribution  
- Mean: `r round(mean(l),1)`
- Median: `r round(median(l),1)`
- Range: `r round(min(l),1)` to `r round(max(l),1)`
- SE: `r round(sd(l),1)`  
]


---
# CLT and the Sampling Distribution

The Central Limit Theorem proves that sample means:   
$$
\bar{y} \sim N(\mu_y,\sigma) 
$$

- Follow a normal distribution 
- Mean = $\mu_y$
- Standard error $\sigma_{\bar{y}} = s_y / \sqrt{n}$
  

---
# So what, CLT?

```{r sclt, fig.width=4, fig.height=3, dpi=350}
  hist(l,
       main = "Sampling distribution",
       xlab = "Sample mean", 
       col="#69b3a2")
  abline(v = mean(l), col = 'red')
  
  
```

- Population mean = `r round(mean(dcps$ProfMath),1)`; Sampling mean = `r round(mean(l),1)`
- SE = `r round(sd(l),1)`
- % sample means within 1.96 SEs: `r round(sum(!is.na(l[l>27-1.96*sd(l) & l < 27+1.96*sd(l)]))/100,1)`


---
class: inverse, middle

# CONFIDENCE INTERVALS

### Embracing uncertainty


---
# Key concepts

- Significance: $\alpha =$ Probability of rejecting true hypothesis. Typically set at $0.05$.

- Confidence: $(1-\alpha) * 100\% =$ probability of accepting a true hypothesis 

- Standard error: Standard deviation of sampling distribution

- Margin of error: Expresses amt of random error for a given level of confidence.  



---
# Building a 95% confidence interval
  
  
$$
\begin{aligned}
CI &= \bar{y} \pm Z_{\alpha/2}  * \sigma_{\bar{y}}\\
95\% CI &= \bar{y} \pm 1.96 * \sigma_{\bar{y}} \\
\end{aligned}
$$
  
- Sample stat: sample mean, $\bar{y}$

- Standard error, sample mean: $\sigma_{\bar{y}} = \frac{s_y}{\sqrt{n}}$

- Margin of error: For 95% confidence: $1.96 * \sigma_{\bar{y}}$ 

- Why it works: 95% of sample means fall w/i 1.96 SEs of the population mean. Only 5% fall outside that range. 


---
# Your own CI

> Follow along for a full example.



---
class: inverse
# Wrapping up

- Sample statistics are variable
- Sampling distributions are well-defined

- Embrace uncertainty with confidence intervals
  - pad your estimate by the margin of error
  

