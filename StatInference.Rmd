---
title: "STATISTICAL INFERENCE"
subtitle: "a guide to the unknown"
author: "Austin Hart"
institute: "American University"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: [default, rladies, rladies-fonts]
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
---


```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  message=FALSE, warning=FALSE, eval = TRUE, echo = FALSE, 
  fig.align = 'center', dev='svglite'
)
```

```{r results='hide'}

library(tidyverse)

load('DCPS testing.RData')

```


# 3 flavors of inference


- **CAUSAL** inference  
  - Attribution of cause to an outcome (X caused Y)  
  - Inference b/c of FPCI  
  
- **STATISTICAL** inference  
  - Educated guess about "population" given sample
  - Inference b/c population parameter unseen/unknowable
  
- **EXTERNAL VALIDITY** inference
  - Educated guess about generalizability of findings
  - Considers other units, treatments, outcomes, settings, etc


---
# Statistical Inference, 2 ways

- **Confidence intervals**: add "margin of error" to construct a range of plausibility around your estimate. 

> I am 95% confidence that the range 10.4 to 11.2 captures the mean level of arsenic in DC public school water fountains.

- **Hypothesis testing**: calculate probability of observing your sample given some assumption about the population

> I reject the hypothesis that DC public school fountains have a "safe" level of arsenic. The probability of observing a mean contamination of 10.8 from a "safe" population is 0.004. 


---
class: inverse, middle

# WHY STATISTICAL INFERENCE?

### Sampling variability


---
# Sampling and the need for inference

> What can data from a single sample tell us about the 
> population from which it was drawn?  
  

- **Population**: entire 'world' of units the study aims to learn about
  - *Parameter*: characteristics of the population, e.g., $\mu_y$  

- **Sample**: 'slice' of data from the population
  - *Sample statistic*: characteristic of sampled units, e.g., $\bar{y}$  

- **Estimation**: sample stat as guide to pop parameter.

- **Sampling Variability**: Every. Slice. Is. Different.



---
# Every sample statistic is different.

Variability in sample statistics

- Different across samples: $\bar{y}_1 \neq \bar{y}_2$  

- Different from target parameter: $\bar{y}_1 \neq \mu_y$


---
# Sampling variability in action

Consider the DCPS math proficiency, 2017

.pull-left[
Population parameters
- Pop size (N) = 108  
- $\mu_y$ = `r round(mean(dcps$ProfMath),1)`  

]

.pull-right[
```{r pop, fig.width=4, fig.height=3, dpi=200}
  hist(dcps$ProfMath,
       main = "Math Proficiency, DCPS 2017",
       xlab = "% testing at grade level", 
       xlim = c(0,100), col="#69b3a2")
  abline(v = mean(dcps$ProfMath), col = 'red')
```

]

---
# Take a sample...

```{r samples}
s1 = pull(dcps, ProfMath) %>% sample(size = 25)
s2 = pull(dcps, ProfMath) %>% sample(size = 25)
```

.pull-left[
### Random Sample 1 (n = 25)
- $\bar{y}_1$ = `r round(mean(s1),1)`

```{r s1, fig.width=4, fig.height=3, dpi=200}
  hist(s1,
       main = "Sample 1",
       xlab = "% testing at grade level", 
       xlim = c(0,100), col="#69b3a2",
       ylim = c(0,10))
  abline(v = mean(s1), col = 'red')
```

]

.pull-right[
### Random Sample 2 (n = 25)
- $\bar{y}_2$ = `r round(mean(s2),1)`

```{r s2, fig.width=4, fig.height=3, dpi=200}
  hist(s2,
       main = "Sample 2",
       xlab = "% testing at grade level", 
       xlim = c(0,100), col="#69b3a2",
       ylim = c(0,10))
  abline(v = mean(s2), col = 'red')
```

]


---
class: inverse, middle

# SAMPLING DISTRIBUTION

### The first step to inference


---
# ALL the sample statistics

> What if we had access to thousands of samples?


- Select a random sample, size 25.  

- Calculate the mean math proficiency.  

- Repeat 10,000 times.  

- Plot the results.


---
# Meet the sampling distribution

> Sampling distribution: distribution of ALL possible/observable sample statistics from a population.


```{r draws}
l = NULL

for (i in 1:10000) {
  l[i] = pull(dcps, ProfMath) %>% sample(size = 25) %>% mean()
}
  
```

.pull-left[

```{r sdist, fig.width=3.5, fig.height=3.5, dpi=200}

  hist(l,
       main = "10,000 sample means",
       xlab = "Sample mean", 
       col="#69b3a2")
  abline(v = mean(l), col = 'red')
  
```
]

.pull-right[  
Our sampling dist  
- Mean: `r round(mean(l),1)`
- Median: `r round(median(l),1)`
- Range: `r round(min(l),1)` to `r round(max(l),1)`
- SD/SE: `r round(sd(l),1)`  
]


---
class: inverse, middle

# CENTRAL LIMIT THEOREM


---
# CLT and the Sampling Distribution

> We don't know if our one sample mean is close to the true population mean.   
> But...   
> the Central Limit Theorem proves that sample means:   
$$
\bar{y} \sim N(\mu_y,\sigma) 
$$

- Follow a normal distribution 
- Mean = $\mu_y$
- Standard error $\sigma_{\bar{y}} = s_y / \sqrt{n}$
  

---
# So what, CLT?

```{r sclt, fig.width=4, fig.height=3, dpi=350}
  hist(l,
       main = "Sampling distribution",
       xlab = "Sample mean", 
       col="#69b3a2")
  abline(v = mean(l), col = 'red')
  
  
```

- Population mean = `r round(mean(dcps$ProfMath),1)`; Sampling mean = `r round(mean(l),1)`
- SE = `r round(sd(l),1)`
- % sample means within 1.96 SEs: `r round(sum(!is.na(l[l>27-1.96*sd(l) & l < 27+1.96*sd(l)]))/100,1)`


---
# If you know 1 normal distribution...

**68-95-99 rule**

- 68% of sample means within $\sim1$ SE of $\mu$
- *95% within $\sim2$ SEs*
- 99.7% within $\sim3$ SEs

```{r sclt2, fig.width=4, fig.height=3, dpi=350}
  ggplot(NULL, aes(c(-3,3))) +
    geom_area(stat = "function", fun = dnorm, fill = "#00998a", xlim = c(-1.96, 1.96)) +
    geom_area(stat = "function", fun = dnorm, fill = "grey80", xlim = c(1.96, 3.5)) +
    geom_area(stat='function', fun = dnorm, fill = 'grey80', xlim = c(-3.5,-1.96)) +
    labs(x = "Z", y = "") +
    scale_y_continuous(breaks = NULL,
                       expand = expansion(mult=c(0,0.05))) +
    scale_x_continuous(breaks = c(-1.96,0,1.96)) +
    coord_cartesian(xlim = c(-3,3)) +
    theme_minimal() +
    theme(panel.grid.minor = element_blank())
```


---
class: inverse, middle

# CONFIDENCE INTERVALS

### Embracing uncertainty


---
# Building a 95% confidence interval
  
  
$$
CI = \bar{y} \pm (1.96*\sigma_{\bar{y}})
$$
  
> Your sample mean is w/i 1.96 SEs of the true population mean.
> The chance I'm wrong? Only 5%.  

- Sample stat: sample mean, $\bar{y}$

- Margin of error: $1.96 * \sigma_{\bar{y}}$ 

- Standard error, sample mean
$$
\sigma_{\bar{y}} = \frac{s_y}{\sqrt{n}}
$$


---
class: inverse, middle

# STATISTICAL INFERENCE
Pulling it all together

---
# Your own CI

> The 2012 ANES survey includes a sample 3,273 white, non-Hispanic adults. 
> The mean resentment index score is 0.66, w/a standard deviation of 0.024.
>
> Calculate and interpret a 95% confidence interval.

--

.pull-left[
- *What we know:*
  - $n = 3,273$
  - $\bar{y}=0.66$
  - $s_y = 0.024$

]

--

.pull-right[
- *What we need:* 
  - Standard error, $\sigma_{\bar{y}}=s_y/\sqrt{n}$ 
  - Margin of error, $ME=1.96*\sigma_{\bar{y}}$
  - Interval $\bar{y} \pm ME$
]
  
---
# Reporting your CI

> The 2012 ANES survey includes a sample 3,273 white, non-Hispanic adults. 
> The mean resentment index score is 0.660, w/a standard deviation of 0.024.
>
> Calculate and interpret a 95% confidence interval.

I am 95% confident that the range 0.652 to 0.668 captures the mean resentment index score for the population of white, non-Hispanic adults in 2012.



---
class: inverse, middle

# HYPOTHESIS TESTING

### Your data versus the status quo

---
# Logic of hypothesis testing

> Given a hypothesis about a population parameter, calculate the probability of finding your sample statistic.  
> Reject the hypothesis when probability is exceptionally low, typically below 0.05. 

Procedure:
1. Specify hypotheses
  - Null hypothesis: status quo expectation
  - Alternative: your expectation  
  
2. Select appropriate test statistic  

3. Specify decision rule for rejecting null  

4. Compute test statistic  

5. Conclude


---
# Setting up hypotheses

- **Alternative hypothesis,** $H_A$
  - Your expectation of the population/truth
  - Empirical implication of your theory

- **Null hypothesis,** $H_0$
  - Pure logical opposite of $H_A$
  - Status quo expectation: you've found nothing new

- TIPS
  - ALWAYS frame hypotheses in terms of parameters
  - Science is all about the null. 
  
  
---
# Select the right test statistic

> A test statistic is a number that standardizes your sample information in light of the null hypothesis.  
> It expresses the deviation of the sample mean from the hypothesized parameter as a number of standard errors.


For a sample mean, you might compute the $Z$-statistic:

$$
Z = \frac{\bar{y}-\mu_0}{s_y/\sqrt{n}}
$$



---
# To reject or not to reject?

### Set a decision rule for rejecting the null  


- Choose a level of significance, typically $\alpha = 0.05$  

- Find the critical value, $Z^*$  
  
- Reject null if test statistic larger than critical value


---
# Finding your critical value, 95% significance

.pull-left[
### Upper-tailed test

Null can only be wrong in one direction.

> Reject $H_0$ if $Z > 1.645$

```{r sclt1s, fig.width=3.5, fig.height=3, dpi=350}
  ggplot(NULL, aes(c(-3.5,3.5))) +
    geom_area(stat = "function", fun = dnorm, fill = "#00998a", xlim = c(-3, 1.645)) +
    geom_area(stat = "function", fun = dnorm, fill = "grey80", xlim = c(1.645, 3.5)) +
    labs(x = "Z", y = "") +
    scale_y_continuous(breaks = NULL,
                       expand = expansion(mult=c(0,0.05))) +
    scale_x_continuous(breaks = c(0,1.645)) +
    coord_cartesian(xlim = c(-3.5,3.5)) +
    theme_minimal() +
    theme(panel.grid.minor = element_blank())
```

]
.pull-right[
### Two-tailed test

Null can be wrong in either direction.

> Reject $H_0$ if $\lvert Z \rvert > 1.96$


```{r sclt2s, fig.width=3.5, fig.height=3, dpi=350}
  ggplot(NULL, aes(c(-3.5,3.5))) +
    geom_area(stat = "function", fun = dnorm, fill = "#00998a", xlim = c(-1.96, 1.96)) +
    geom_area(stat = "function", fun = dnorm, fill = "grey80", xlim = c(1.96, 3.5)) +
    geom_area(stat='function', fun = dnorm, fill = 'grey80', xlim = c(-3.5,-1.96)) +
    labs(x = "Z", y = "") +
    scale_y_continuous(breaks = NULL,
                       expand = expansion(mult=c(0,0.05))) +
    scale_x_continuous(breaks = c(-1.96,0,1.96)) +
    coord_cartesian(xlim = c(-3.5,3.5)) +
    theme_minimal() +
    theme(panel.grid.minor = element_blank())
```
]


---
# Your own hypothesis test

> The 2012 ANES survey includes a sample 3,273 white, non-Hispanic adults. 
> The mean resentment index score is 0.660, and the standard deviation is 0.024.
>
> Evaluate the null hypothesis that the population is racially neutral or liberal on this scale.




---
# Present your hypothesis test

> The mean resentment index score equals 0.66. It is highly unlikely that we see this in a racially neutral or liberal population $(Z = xx)$ and we conclude that the population mean is significantly more conservative on this scale. 
>

---
class: inverse
# Wrapping up

- Sample statistics are variable
- Sampling distributions are well-defined

- Embrace uncertainty with confidence intervals
  - pad your estimate by the margin of error
  
- Test specific hypotheses about the population
  - reject (null) hypotheses that are unlikely to have produced your data 
