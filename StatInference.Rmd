---
title: "STATISTICAL INFERENCE"
subtitle: "a guide to the unknown"
author: "Austin Hart"
institute: "American University"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: [default, rladies, rladies-fonts]
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---


```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  message=FALSE, warning=FALSE, eval = TRUE, echo = FALSE, 
  fig.align = 'center', dev='svglite'
)
```

```{r results='hide'}
library(tidyverse)
library(magrittr)
library(kableExtra)

load('DCPS testing.RData')
load('anes2012 workshop.RData')
```


# 3 flavors of inference


- **CAUSAL** inference  
  - Attribution of cause to an outcome (X caused Y)  
  - Inference b/c of FPCI  
  
- **STATISTICAL** inference  
  - Educated guess about "population" given sample
  - Inference b/c population parameter unseen/unknowable
  
- **EXTERNAL VALIDITY** inference
  - Educated guess about generalizability of findings
  - Considers other units, treatments, outcomes, settings, etc


---
# Statistical Inference, 2 ways

1. **Confidence intervals**: add "margin of error" to construct a range of plausibility around your estimate. 

> I am 95% confidence that the range 10.4 to 11.2 captures the mean level of arsenic in DC public school water fountains.

2. **Hypothesis testing**: calculate probability of observing your sample given some assumption about the population

> I reject the hypothesis that DC public school fountains have a "safe" level of arsenic. The probability of observing a mean contamination of 10.8 from a "safe" population is 0.004. 


---
class: inverse, middle

# WHY STATISTICAL INFERENCE?

### Sampling variability


---
# Sampling and the need for inference

> What can data from a single sample tell us about the 
> population from which it was drawn?  
  

- **Population**: entire 'world' of units the study aims to learn about

- **Parameter**: characteristics of the population, e.g., $\mu_y$

- **Sample**: 'slice' of data from the population

- **Sample statistic**: characteristic of sampled units, e.g., $\bar{y}$

- **Estimation**: sample stat as guide to pop parameter.

- **Sampling Variability**: Every. Slice. Is. Different.



---
# Every sample statistic is different.

Variability in sample statistics

- Different across samples: $\bar{y}_1 \neq \bar{y}_2$

- Different from target parameter: $\bar{y}_1 \neq \mu_y$


---
# Sampling variability in action

Consider the DCPS math proficiency, 2017

.pull-left[
Population parameters
- Pop size (N) = 108  
- $\mu_y$ = `r round(mean(dcps$ProfMath),1)`  

]

.pull-right[
```{r pop, fig.width=3.5, fig.height=3, dpi=200}
  hist(dcps$ProfMath,
       main = "Math Proficiency, DCPS 2017",
       xlab = "% testing at grade level", 
       xlim = c(0,100), col="#69b3a2")
  abline(v = mean(dcps$ProfMath), col = 'red')
```

]

---
# Take a sample...

```{r samples}
s1 = pull(dcps, ProfMath) %>% sample(size = 25)
s2 = pull(dcps, ProfMath) %>% sample(size = 25)
```

.pull-left[
### Random Sample 1 (n = 25)
- $\bar{y}_1$ = `r round(mean(s1),1)`

```{r s1, fig.width=3.5, fig.height=3, dpi=200}
  hist(s1,
       main = "Sample 1",
       xlab = "% testing at grade level", 
       xlim = c(0,100), col="#69b3a2")
  abline(v = mean(s1), col = 'red')
```

]

.pull-right[
### Random Sample 2 (n = 25)
- $\bar{y}_2$ = `r round(mean(s2),1)`

```{r s2, fig.width=3.5, fig.height=3, dpi=200}
  hist(s2,
       main = "Sample 2",
       xlab = "% testing at grade level", 
       xlim = c(0,100), col="#69b3a2")
  abline(v = mean(s2), col = 'red')
```

]


---
class: inverse, middle

# SAMLING DISTRIBUTION

### The first step to inference


---
# ALL the sample statistics

- Select a random sample, size 25.
- Calculate the mean math proficiency.
- Repeat 20,000 times.
- Plot the results.


---
# Meet the sampling distribution

> Sampling distribution: distribution of ALL possible/observable sample statistics from a population.


```{r draws}
l = NULL

for (i in 1:20000) {
  l[i] = pull(dcps, ProfMath) %>% sample(size = 25) %>% mean()
}
  
```

.pull-left[

```{r sdist, fig.width=3.5, fig.height=3.5, dpi=200}

  hist(l,
       main = "20,000 sample means",
       xlab = "Sample mean", 
       col="#69b3a2")
  abline(v = mean(l), col = 'red')
  
```
]

.pull-right[  
Our sampling dist  
- Mean: `r round(mean(l),1)`
- Median: `r round(median(l),1)`
- Range: `r round(min(l),1)` to `r round(max(l),1)`
- SD/SE: `r round(sd(l),1)`  
]


---
class: inverse, middle

# CENTRAL LIMIT THEOREM


---
# CLT and the Sampling Distribution

>Sampling variability highlights the uncertainty of inference: we don't know if our sample stat is close to the target parameter.   
>But...   
>the Central Limit Theorem proves that:  
>
$$
\bar{y} \sim N(\mu_y,\sigma) 
$$

Meaning that the sampling distribution of sample means

- Follows a normal distribution 
- Mean = $\mu_y$
- Standard error $\sigma_{\bar{y}} = s_y / \sqrt{n}$
  

---
# So what, CLT?

```{r sclt, fig.width=4, fig.height=3, dpi=350}
  hist(l,
       main = "Sampling distribution",
       xlab = "Sample mean", 
       col="#69b3a2")
  abline(v = mean(l), col = 'red')
  
```

- Population mean = 27; Sampling mean = `r round(mean(l),1)`
- SE = `r round(sd(l),1)`
- % sample means within 2 SEs: `r round(sum(!is.na(l[l>27-2*sd(l) & l < 27+2*sd(l)]))/200,1)`


---
# If you know 1 normal distribution...

**68-95-99 rule**

- 68% of sample means within $\sim1$ SE of $\mu$
- *95% within $\sim2$ SEs*
- 99.7% within $\sim3$ SEs

```{r sclt2, fig.width=4, fig.height=3, dpi=350}
  hist(l,
       main = "Sampling distribution",
       xlab = "Sample mean", 
       col="#69b3a2")
  abline(v = mean(l), col = 'red')
  abline(v = mean(l)-2*sd(l), col = 'blue')
  abline(v = mean(l)+2*sd(l), col = 'blue')
  
```


---
class: inverse, middle

# CONFIDENCE INTERVALS

### Embracing uncertainty


---
# Building a confidence interval
  
  
$$
CI = \bar{y} \pm (1.96*\sigma_{\bar{y}})
$$
  
> Your sample mean is w/i about 1.96 SEs of the true population mean.
> The chance I'm wrong? Only 5%.  

- Sample stat: sample mean, $\bar{y}$, or proportion, $\hat{p}$  

- Margin of error: $1.96 * \sigma_{\bar{y}}$ 



---
# Knowing your margin of error

- SE, Sample mean
$$
\sigma_{\bar{y}} = \frac{s_y}{\sqrt{n}}
$$

- SE, Sample proportion  
$$
\sigma_{\hat{p}} = \sqrt{\frac{\hat{p}(1-\hat{p})}{n}}
$$
  
- Critical statistic $(Z,t)$
  - 68% confidence: 
  - 95% confidence: 
  - 99.7% confidence: 
  

---
class: inverse, middle

# STATISTICAL INFERENCE
Pulling it all together

---
# Your own CI

> The 2012 ANES survey includes a sample 3,273 white, non-Hispanic adults. 
> The mean resentment index score is 0.66, w/a standard deviation of 0.24.
>
> Calculate and interpret a 95% confidence interval.

--

.pull-left[
- *What we know:*
  - n = 3,273
  - $\bar{y}=0.66$
  - $s_y = 0.24$

]

--

.pull-right[
- *What we need:* $\bar{y} \pm (1.96*\sigma_{\bar{y}})$
  - Standard error, $\sigma_{\bar{y}}=s_y/\sqrt{n}$ 
  - Margin of error, $ME=1.96*\sigma_{\bar{y}}$
  - Interval $\bar{y} \pm ME$
]
  
---
# Reporting your CI

> The 2012 ANES survey includes a sample 3,273 white, non-Hispanic adults. 
> The mean resentment index score is 0.66, w/a standard deviation of 0.24.
>
> Calculate and interpret a 95% confidence interval.

I am 95% confident that the range 0.652 to 0.668 captures the mean resentment index score for the population of white, non-Hispanic adults in 2012.



---
class: inverse, middle

# HYPOTHESIS TESTING

### Your data versus the status quo

---
# Logic of hypothesis testing

### (Null) hypothesis testing

> Calculating the probability of your sample data given a hypothesis about the population.  
> That probability is the basis for rejecting or not rejecting that hypothesis.

Procedure:
- Specify hypotheses
  - Null hypothesis: status quo expectation
  - Alternative: your expectation
- Calculate $Pr(data|H_0)$; e.g., $Pr(\bar{y} \geq 7.2 | \mu_y = 0)$



---
# Setting up hypotheses

- Alternative hypothesis, $H_A$
  - *Your* expectation of the population/truth
  - Empirical implication of your theory

- Null hypothesis, $H_0$
  - Pure logical opposite of $H_A$
  - Status quo expectation: you've found nothing new

- TIPS
  - ALWAYS frame hypotheses in terms of parameters
  - Science is all about the null. 
  
  
---
# To reject or not to reject?

> On what grounds can I reject the null hypothesis?

- Default position: status quo wins
  - fail to reject the null
  - Stick with conventional wisdom
  
- Reject the null
  - *iff* sample stat is totally implausible given $H_0$
  - typically $p \leq 0.05$


---
# The meaning of a p-value

> If the conventional wisdom is true, what share of samples reveal a stat or association like yours? 


$$
p = Pr(Sample~Stat | H_0)
$$



```{r scltht, fig.width=4, fig.height=3, dpi=350}
  hist(l,
       main = "Hypothesized sampling distribution",
       xlab = "Sample mean", 
       col="#69b3a2")
  abline(v = mean(l), col = 'red')
  abline(v = mean(l)+0.8*sd(l), col = 'orange',lt=3)
  abline(v = mean(l)+2.5*sd(l), col = 'blue')
  
```


---
# Your own CI

> The 2012 ANES survey includes a sample 3,273 white, non-Hispanic adults. 
> The mean resentment index score is 0.66.
>
> Evaluate the null hypothesis that the population is racially neutral or liberal on this scale.




---
# Present your hypothesis test

> The mean resentment index score among white, non-Hispanic respondents in the 2012 ANES equals 0.66. It is highly unlikely that we see this in a racially neutral or liberal population $(t = xx$, $p = xx)$ and we conclude that the population mean is significantly more conservative than the neutral point on this scale. 
>

---
class: inverse
# Wrapping up

- Sample statistics are variable
- Sampling distributions are well-defined

- Embrace uncertainty with confidence intervals
  - pad your estimate by the margin of error
  
- Test specific hypotheses about the population
  - find probability of your sample given status quo expectation (null)
